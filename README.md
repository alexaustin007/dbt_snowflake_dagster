# dbt_snowflake_dagster

# DBT, Snowflake, and Dagster Integration

This project demonstrates the integration of DBT, Snowflake, and Dagster for a seamless data pipeline. The goal is to set up a data workflow that extracts data, loads it into Snowflake, and runs a DBT model to process the data.

## Project Structure

The project is organized as follows:

- **dagster_project**: Contains the Dagster configuration, pipelines, assets, and job definitions.
  - **my_dagster_project**: Main folder for Dagster project logic, including jobs and assets.
  - **logs**: Logs generated by the Dagster pipeline runs.
  
- **dbt_project**: Contains the DBT configuration and models for data transformation.
  - **models**: Contains DBT models for data transformation.
  - **dbt_project.yml**: Configuration for the DBT project.

- **docker**: Docker-related files for containerizing the application.
  - **Dockerfile.dagster**: Dockerfile for the Dagster environment.
  - **Dockerfile_dbt**: Dockerfile for the DBT environment.
  - **docker-compose.yml**: Docker Compose configuration to run the entire setup.

- **README.md**: This README file.

## Setup Instructions

### Prerequisites

Before you begin, ensure you have the following tools installed:
- Docker
- Docker Compose
- Python 3.8+ (for the Dagster and DBT environments)
- Snowflake account for database integration
- Git

### Steps to Run the Project

1. **Clone the repository**:

   First, clone the repository to your local machine:
   ```bash
   git clone https://github.com/alexaustin007/dbt_snowflake_dagster.git
   cd dbt_snowflake_dagster
Configure the .env file:

Create a .env file to store your environment variables, such as Snowflake credentials. For example:


SNOWFLAKE_USER=your_snowflake_username

SNOWFLAKE_PASSWORD=your_snowflake_password

SNOWFLAKE_ACCOUNT=your_snowflake_account

SNOWFLAKE_WAREHOUSE=your_snowflake_warehouse

SNOWFLAKE_DATABASE=your_snowflake_database

SNOWFLAKE_SCHEMA=your_snowflake_schema

Run Docker Compose:

Use Docker Compose to set up the Dagster and DBT containers.


docker-compose up --build
This will build and start all necessary containers for Dagster, DBT, and the Snowflake database.

Run the Dagster pipeline:

Start dagster by "dagster dev" command, After Docker Compose has started, open Dagster's UI at http://localhost:3000. You can run the job defined in the my_dagster_project/my_jobs.py file.

Monitor the job in Dagster:

From the Dagster UI, you can monitor the pipeline runs, check for logs, and see the results of the job.

DBT Models:

The DBT models will be executed as part of the pipeline in Dagster, and the results will be loaded into Snowflake.

Push to GitHub:

After making changes or updates to the project, commit and push them to GitHub:


git add .
git commit -m "Update pipeline and configuration"
git push origin main
